{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "War5iAnjFDfz"
      },
      "source": [
        "### **Student Information**\n",
        "Name: 楊婷伃\n",
        "\n",
        "Student ID: 111071046\n",
        "\n",
        "GitHub ID: annie111071046\n",
        "\n",
        "Kaggle name: annietyyang\n",
        "\n",
        "Kaggle private scoreboard snapshot:\n",
        "\n",
        "![pic_ranking.png](./pics/pic_ranking.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjJdSbPtFDf1"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZk_MjJoFDf2"
      },
      "source": [
        "# **Instructions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo3YJb6SFDf2"
      },
      "source": [
        "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
        "\n",
        "**Environment recommendations to solve lab 2:**\n",
        "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
        "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
        "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
        "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models.\n",
        "\n",
        "## **Phase 1 (30 pts):**\n",
        "\n",
        "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
        "\n",
        "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**.\n",
        "\n",
        "## **Phase 2 (30 pts):**\n",
        "\n",
        "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
        "\n",
        "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**.\n",
        "\n",
        "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
        "\n",
        "## **Phase 3 (40 pts):**\n",
        "\n",
        "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking:\n",
        "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
        "\n",
        "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
        "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
        "\n",
        "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements:\n",
        "* Your preprocessing steps.\n",
        "* The feature engineering steps.\n",
        "* Explanation of your model.\n",
        "\n",
        "* **`Bonus (5 pts):`**\n",
        "    * You will have to describe more detail in the previous steps.\n",
        "    * Mention different things you tried.\n",
        "    * Mention insights you gained.\n",
        "\n",
        "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
        "\n",
        "**`Things to note for Phase 3:`**\n",
        "\n",
        "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
        "\n",
        "* **Push the code used for the competition to your repository**.\n",
        "\n",
        "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
        "\n",
        "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
        "\n",
        "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
        "\n",
        "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
        "\n",
        "## **Deadlines:**\n",
        "\n",
        "![lab2_deadlines](./pics/lab2_deadlines.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7xhFu0XFDf3"
      },
      "source": [
        "---\n",
        "\n",
        "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
        "\n",
        "You can delete the syntax suggestions after you use them.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4J8J-6YeMuYj"
      },
      "source": [
        "***\n",
        "\n",
        "# **Project Report**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Model Development (10 pts Required)**\n",
        "\n",
        "In this competition, I implemented two approaches to emotion classification. I started with a strong baseline using TF-IDF + Logistic Regression to understand the data's characteristics, and then moved to a Transformer-based model (RoBERTa) to capture deep semantic context and improve performance.\n",
        "\n",
        "### **1.1 Preprocessing Steps**\n",
        "\n",
        "I used two preprocessing method, one for Logistic Regression, and one for RoBERTa.\n",
        "\n",
        "* 1.1.1 Merge data :\n",
        "  I merged everything I need into a dataset called `df_full`.\n",
        "\n",
        "* 1.1.2 Transform text format :\n",
        "\n",
        "  I created a `clean_text` function to normalize the data, including converting all text to **lowercase** and removing **links**, **HTML tags**, **user mentions (@user)**, **special characters**, and **extra whitespaces**.\n",
        "\n",
        "### **1.2 Feature Engineering Steps**\n",
        "\n",
        "* 1.2.1 TF-IDF (for Logistic Regression):\n",
        "\n",
        "  I used `TfidfVectorizer` to convert text into numerical vectors. I limited the vocabulary to the **top 5,000** most frequent features (max_features=5000) and removed standard English **stop words** to reduce dimensionality and filter out non-informative high-frequency words.\n",
        "* 1.2.2 Label Mapping & Class Weights (for RoBERTa):\n",
        "  * Label Encoding :\n",
        "  \n",
        "    I mapped the categorical emotion labels (e.g., \"joy\") to numerical ones (0, 1, 2...), in order to be compatible with the BERT architecture.\n",
        "  * Handling Imbalance :\n",
        "\n",
        "    I found out that the dataset is highly imbalanced (\"joy\" is dominant). To solve this, I calculated **Inverse Class Weights** using the formula: `Weight = Total_Samples / (Num_Classes * Class_Count)`. These weights were passed to the model's loss function to penalize misclassifications of minority classes more heavily.\n",
        "\n",
        "### **1.3 Explanation of Your Model**\n",
        "\n",
        "* 1.3.1 Logistic Regression\n",
        "\n",
        "  This is a linear classifier. I set `class_weight='balanced'` to automatically adjust weights based on class frequencies.\n",
        "\n",
        "* 1.3.2 RoBERTa (Robustly Optimized BERT Pretraining Approach)\n",
        "\n",
        "  I chose **RoBERTa** because it generally outperforms BERT on sentiment tasks due to its optimized training on larger corpora.\n",
        "  * Epochs:\n",
        "\n",
        "    I trained 3 and 4 epochs to ensure sufficient convergence.\n",
        "\n",
        "  * Sequence Length:\n",
        "  \n",
        "    Set to 128 to cover the length of most tweets.\n",
        "  \n",
        "  * Custom Loss Weights:\n",
        "  \n",
        "    I put the pre-calculated class weights into the model to explicitly address the data imbalance problem.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Bonus Section (5 pts Optional)**\n",
        "\n",
        "**Add more detail in previous sections**\n",
        "\n",
        "### **2.1 Mention Different Things You Tried**\n",
        "\n",
        "* Models:\n",
        "\n",
        "  I experimented with two main architectures: **Logistic Regression** and **RoBERTa**. For RoBERTa, I specifically tuned the `num_train_epochs` parameter. I trained the model for **3 epochs** and **4 epochs** to observe convergence.\n",
        "\n",
        "### **2.2 Mention Insights You Gained**\n",
        "\n",
        "* 2.2.1 Compare Result\n",
        "\n",
        "  From the images below, we can see that the performances of the three models are:\n",
        "\n",
        "  * Logistic Regression\n",
        "    * **Model Report:**\n",
        "\n",
        "      ![colab_lr](./pics/colab_lr.png)\n",
        "    * **Competition Score**\n",
        "    \n",
        "      ![kaggle_lr](./pics/kaggle_lr.png)\n",
        "    * **Accuracy:** 0.51\n",
        "    * **Macro F1-score:** 0.41\n",
        "    * The model struggled significantly with minority classes. For instance, \"disgust\" had a very low F1-score of **0.17**, and \"fear\" was only **0.38**. This shows that a simple Bag-of-Words approach fails to capture the nuances of rare emotions.\n",
        "\n",
        "  * RoBERTa (with epoch=3)\n",
        "    * **Model Report:**\n",
        "    \n",
        "      ![colab_bert_epoch3](./pics/colab_bert_epoch3.png)\n",
        "    * **Competition Score:**\n",
        "\n",
        "      ![kaggle_bert_epoch3](./pics/kaggle_bert_epoch3.png)\n",
        "    * **Accuracy:** 0.653\n",
        "    * **Macro F1-score:** 0.54\n",
        "    * Switching to a Transformer model yielded a big improvement. The model's ability to understand context helped it distinguish emotions better. Notably, \"fear\" improved from 0.38 to **0.55**, and \"joy\" even reached **0.79**.\n",
        "\n",
        "  * RoBERTa (with epoch=4)\n",
        "    * **Model Report:**\n",
        "\n",
        "      ![colab_bert_epoch4](./pics/colab_bert_epoch4.png)\n",
        "    * **Competition Score:**\n",
        "\n",
        "      ![kaggle_bert_epoch4](./pics/kaggle_bert_epoch4.png)\n",
        "    * **Accuracy:** 0.651\n",
        "    * **Macro F1-score:** 0.53\n",
        "    * Training for an extra epoch **didn't** lead to better results. In fact, the accuracy remained almost flat (or even dropped slightly), and the Macro F1-score decreased from 0.54 to 0.53. Specific classes like \"disgust\" dropped slightly (0.27 -> 0.25).\n",
        "\n",
        "* 2.2.2 Insights\n",
        "  * **Transformers > TF-IDF:** RoBERTa significantly outperforms Logistic Regression because it captures semantic meaning rather than just keyword frequency.\n",
        "  * **Convergence & Overfitting:** The comparison between epoch 3 and epoch 4 suggests that the model **converged early**. Training for longer (epoch 4) didn't help and might have started to slightly overfit the training data or bias towards majority classes, as seen in the drop in minority class performance. Epoch 3 was the most efficient and effective setting.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfVVozXzFDf4"
      },
      "source": [
        "**`From here on starts the code section for the competition.`**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SomELteEFDf5"
      },
      "source": [
        "# **Competition Code**\n",
        "\n",
        "# 1. Preprocessing Steps"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install simpletransformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QXV8K9x2wqaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9la7ugKFDf5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
        "import logging\n",
        "import torch\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "df_id = pd.read_csv('/content/data_identification.csv')\n",
        "df_emotion = pd.read_csv('/content/emotion.csv')\n",
        "with open('/content/final_posts.json', 'r', encoding='utf-8') as f:\n",
        "    posts_data = json.load(f)\n",
        "\n",
        "#parse json file\n",
        "parsed_posts = []\n",
        "for entry in posts_data:\n",
        "    try:\n",
        "        post_info = entry['root']['_source']['post']\n",
        "        parsed_posts.append({\n",
        "            'id': post_info['post_id'],\n",
        "            'text': post_info['text']\n",
        "        })\n",
        "    except KeyError:\n",
        "        continue\n",
        "\n",
        "df_posts = pd.DataFrame(parsed_posts)\n",
        "\n",
        "# merge data\n",
        "df_full = pd.merge(df_posts, df_id, on='id', how='left')\n",
        "df_full = pd.merge(df_full, df_emotion, on='id', how='left')\n",
        "df_full.head()"
      ],
      "metadata": {
        "id": "HjjeuldqwejJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean texts\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) # remove links\n",
        "    text = re.sub(r'<.*?>', '', text) # remove HTML\n",
        "    text = re.sub(r'@\\w+', '', text)  # remove Mentions\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text) # Only letters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() #remove extra whitespaces\n",
        "    return text\n",
        "df_full['clean_text'] = df_full['text'].apply(clean_text)\n",
        "df_full['clean_text'].head()"
      ],
      "metadata": {
        "id": "rzpTbR2mxGZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wc2D8apFDf7"
      },
      "source": [
        "# 2. Feature Engineering Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 TFIDF (for Logistic Regression)"
      ],
      "metadata": {
        "id": "WatHPFKuyA3H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyL9Ls05FDf7"
      },
      "outputs": [],
      "source": [
        "# Split data for Logistic Regression\n",
        "df_train_lr = df_full[df_full['split'] == 'train'].dropna(subset=['emotion'])\n",
        "df_test_lr = df_full[df_full['split'] == 'test']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# feature extraction\n",
        "tfidf = TfidfVectorizer(max_features=5000, stop_words='english') # top 5000 words\n",
        "X_lr = tfidf.fit_transform(df_train_lr['clean_text'])\n",
        "y_lr = df_train_lr['emotion']\n",
        "X_submission_lr = tfidf.transform(df_test_lr['clean_text'])\n",
        "print(X_submission_lr)"
      ],
      "metadata": {
        "id": "A_RdUWjH4H2Z",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Label Mapping (for BERT)"
      ],
      "metadata": {
        "id": "bxxjWbTcyqu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Label Mapping\n",
        "label_list = sorted(df_full['emotion'].dropna().unique().tolist())\n",
        "label_map = {label: i for i, label in enumerate(label_list)}\n",
        "inverse_label_map = {i: label for label, i in label_map.items()}\n",
        "inverse_label_map"
      ],
      "metadata": {
        "id": "7mcCpAoXyvIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format Data for SimpleTransformers (text, labels)\n",
        "df_train_bert = df_full[df_full['split'] == 'train'].dropna(subset=['emotion']).copy()\n",
        "df_test_bert = df_full[df_full['split'] == 'test'].copy()\n",
        "\n",
        "df_train_bert['labels'] = df_train_bert['emotion'].map(label_map)\n",
        "train_df = df_train_bert[['text', 'labels']]\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "FSWWWCrmyy8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Class Weights for Imbalance\n",
        "class_counts = train_df['labels'].value_counts().sort_index().values\n",
        "num_labels = len(label_list)\n",
        "total_samples = len(train_df)\n",
        "class_weights = total_samples / (num_labels * class_counts)\n",
        "weights_list = class_weights.tolist()\n",
        "\n",
        "print(f\"Label Map: {label_map}\")\n",
        "print(f\"Class Weights: {weights_list}\")"
      ],
      "metadata": {
        "id": "i6tyibTyzFmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cebfGDHyFDf8"
      },
      "source": [
        "# 3. Model Implementation Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Logistic Regression"
      ],
      "metadata": {
        "id": "NwwyT6qEzh7g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0avhp7yVFDf8"
      },
      "outputs": [],
      "source": [
        "# train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_lr, y_lr, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train\n",
        "lr_model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# validate\n",
        "test_preds = lr_model.predict(X_test)\n",
        "print(\"Baseline Accuracy:\", accuracy_score(y_test, test_preds))\n",
        "print(classification_report(y_test, test_preds))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# full result\n",
        "lr_model.fit(X_lr, y_lr)\n",
        "test_predictions = lr_model.predict(X_submission_lr)\n",
        "submission_lr = pd.DataFrame({\n",
        "    'id': df_test_lr['id'],\n",
        "    'emotion': test_predictions\n",
        "})\n",
        "submission_lr"
      ],
      "metadata": {
        "id": "45ku8JfU5ZpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download result\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "submission_lr.to_csv('submission_lr.csv', index=False)\n",
        "files.download('submission_lr.csv')"
      ],
      "metadata": {
        "id": "wauq1EB_7LKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 BERT"
      ],
      "metadata": {
        "id": "DssAC_4Nzu2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split for validation (train:80%, test:20%)\n",
        "train_df_split, eval_df_split = train_test_split(train_df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "rX0rz96P1abs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set arguments\n",
        "model_args = ClassificationArgs()\n",
        "model_args.num_train_epochs = 4   #I change this: train 3 or 5 rounds\n",
        "model_args.train_batch_size = 16\n",
        "model_args.eval_batch_size = 32\n",
        "model_args.learning_rate = 4e-5\n",
        "model_args.max_seq_length = 128   #set max length of a sentence\n",
        "model_args.overwrite_output_dir = True\n",
        "model_args.save_model_every_epoch = False\n",
        "model_args.use_multiprocessing = False\n",
        "model_args.use_multiprocessing_for_evaluation = False"
      ],
      "metadata": {
        "id": "LiV9UTEpzzOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize model\n",
        "model = ClassificationModel(\n",
        "    'roberta',\n",
        "    'roberta-base',\n",
        "    num_labels=num_labels,\n",
        "    args=model_args,\n",
        "    weight=weights_list, # class weights\n",
        "    use_cuda=torch.cuda.is_available()\n",
        ")"
      ],
      "metadata": {
        "id": "ct9mrzk21g_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train(1 epoch takes about 7 mins to finish)\n",
        "model.train_model(train_df_split)"
      ],
      "metadata": {
        "id": "ny7RzkWB1oIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation\n",
        "result, model_outputs, wrong_predictions = model.eval_model(eval_df_split)\n",
        "preds_bert = model_outputs.argmax(axis=1)\n",
        "print(\"RoBERTa Accuracy:\", accuracy_score(eval_df_split['labels'], preds_bert))\n",
        "print(classification_report(eval_df_split['labels'], preds_bert, target_names=label_list))"
      ],
      "metadata": {
        "id": "r7SsgiaZ1rzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# result(this takes about 1 min to finish)\n",
        "predictions, _ = model.predict(df_test_bert['text'].tolist())\n",
        "final_labels = [inverse_label_map[pred] for pred in predictions]\n",
        "\n",
        "submission_bert = pd.DataFrame({\n",
        "    'id': df_test_bert['id'],\n",
        "    'emotion': final_labels\n",
        "})\n",
        "submission_bert"
      ],
      "metadata": {
        "id": "yWGm5ayU1wpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download result\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "submission_bert.to_csv('submission_bert.csv', index=False)\n",
        "files.download('submission_bert.csv')"
      ],
      "metadata": {
        "id": "s4uaAc8Z2kCo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}